{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports huggingface transformers\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: gpt2\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 pipline\n",
    "gpt2 = transformers.pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentences\n",
    "test1 = \"\"\"\n",
    "I am happy. I am happy.\n",
    "I am sad. I am sad.\n",
    "I am sad. I am sad.\n",
    "I am happy. I am happy.\n",
    "I am sad. I am sad.\n",
    "I am happy. I am happy.\n",
    "I am happy. I am happy.\n",
    "I am happy. I am happy.\n",
    "I am happy. I am happy.\n",
    "I am sad. I am sad.\n",
    "I am happy. I am\"\"\"\n",
    "test2 = \"\"\"\n",
    "I am happy. I am happy.\n",
    "I am sad. I am sad.\n",
    "I am sad. I am sad.\n",
    "I am happy. I am happy.\n",
    "I am sad. I am sad.\n",
    "I am happy. I am happy.\n",
    "I am happy. I am happy.\n",
    "I am happy. I am happy.\n",
    "I am happy. I am happy.\n",
    "I am sad. I am sad.\n",
    "I am sad. I am\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam']\n",
      "['Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġhappy', '.', 'ĠI', 'Ġam', 'Ġhappy', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam', 'Ġsad', '.', 'Ċ', 'I', 'Ġam', 'Ġsad', '.', 'ĠI', 'Ġam']\n"
     ]
    }
   ],
   "source": [
    "# view tokenizations\n",
    "print(tokenizer.tokenize(test1))\n",
    "print(tokenizer.tokenize(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average encodings!\n",
    "enc1 = tokenizer.encode(test1)\n",
    "enc2 = tokenizer.encode(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 3772, 13, 314, 716] \n",
      " [198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 3772, 13, 314, 716, 3772, 13, 198, 40, 716, 6507, 13, 314, 716, 6507, 13, 198, 40, 716, 6507, 13, 314, 716]\n",
      "97 97\n"
     ]
    }
   ],
   "source": [
    "print(enc1, \"\\n\", enc2)\n",
    "print(len(enc1), len(enc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets token embeddings\n",
    "embedding_matrix = model.get_input_embeddings()\n",
    "embeddings1 = embedding_matrix(torch.tensor(enc1)).detach().numpy()\n",
    "embeddings2 = embedding_matrix(torch.tensor(enc2)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averages embeddings\n",
    "# embeddings_avg = (embeddings1 + embeddings2) / 2\n",
    "embeddings_avg = embeddings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jstav/miniconda3/envs/nlu/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If inputs_embeds is passed as model-specific keyword input then model has to be an encoder-decoder and not a GPT2LMHeadModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[255], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(inputs_embeds\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m      4\u001b[0m \u001b[39m# decoder_input_ids = torch.ones((inputs_embeds.shape[0], 1), dtype=torch.long)*model.config.decoder_start_token_id\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(attention_mask\u001b[39m=\u001b[39;49mattention_mask, inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/transformers/generation/utils.py:1220\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     generation_config\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m eos_token_id\n\u001b[1;32m   1215\u001b[0m \u001b[39m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1220\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_model_inputs(\n\u001b[1;32m   1221\u001b[0m     inputs, generation_config\u001b[39m.\u001b[39;49mbos_token_id, model_kwargs\n\u001b[1;32m   1222\u001b[0m )\n\u001b[1;32m   1223\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1225\u001b[0m \u001b[39m# 4. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu/lib/python3.9/site-packages/transformers/generation/utils.py:535\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[0;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m# 4. Only encoder-decoder models can have non `input_ids` input format\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m input_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    536\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf \u001b[39m\u001b[39m{\u001b[39;00minput_name\u001b[39m}\u001b[39;00m\u001b[39m is passed as model-specific keyword \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput then model has to be an encoder-decoder and not a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n\u001b[1;32m    541\u001b[0m \u001b[39m# 5. if `inputs` is still None, try to create `input_ids` from BOS token\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: If inputs_embeds is passed as model-specific keyword input then model has to be an encoder-decoder and not a GPT2LMHeadModel."
     ]
    }
   ],
   "source": [
    "# copy-pasted?!\n",
    "inputs_embeds = embeddings_avg\n",
    "attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long)\n",
    "# decoder_input_ids = torch.ones((inputs_embeds.shape[0], 1), dtype=torch.long)*model.config.decoder_start_token_id\n",
    "output_ids = model.generate(attention_mask=attention_mask, inputs_embeds=inputs_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls LM on averaged embeddings\n",
    "output = model(inputs_embeds=torch.tensor(embeddings_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'m not to I am happy. I\n",
      " am happy. I am sad.\n",
      "I am sad. I am sad.\n",
      "I am sad. I am happy.\n",
      "I am happy. I am happy.\n",
      "I am sad. I am happy.\n",
      "I am happy. I am happy.\n",
      "I am happy. I am happy.\n",
      "I am happy. I am happy.\n",
      "I am happy. I am sad.\n",
      "I am sad. I am happy\n"
     ]
    }
   ],
   "source": [
    "# view model completions\n",
    "print(tokenizer.decode(output.logits.argmax(dim=-1).squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in am rather to I am a that I\n",
      " am happy I I am m.\n",
      "I am happy. I am sad.\n",
      "And am sad. I am happy.\n",
      "I am happy. I am sad.\n",
      "I am happy. I am happy.\n",
      "I am sad. I am happy.\n",
      "I am happy. I am happy.\n",
      "Tell am happy. I am happy.\n",
      "I am happy. I am sad.\n",
      "I am sad. I am happy\n"
     ]
    }
   ],
   "source": [
    "# samples random completion\n",
    "logits = output.logits\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "next_token = torch.multinomial(probs, num_samples=1)\n",
    "print(tokenizer.decode(next_token.squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  13,   15,   13,  352, 1849])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.argmax(dim=1).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
